{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "The normalization step of directlfq is adapted from the [MS-EmpiRe algorithm](https://doi.org/10.1074/mcp.RA119.001509). It aims at reducing systematic biases between samples. Such biases can for example occur when more material is pipetted in one of the samples. In principle, two steps are performed:\n",
    "\n",
    "1. Normalize between samples of the same condition (i.e. replicates)\n",
    "\n",
    "2. Normalize between different conditions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-condition normalization\n",
    "It is common practice and highly recommended to measure multiple samples of a given condition. This ensures that observed changes between conditions are not just due to random variation. Examples of samples within the same condition could be biological replicates, but also patients with the same clinical condition. \n",
    "We want to ensure that systematic changes between within-condition samples are corrected for as follows:\n",
    "\n",
    "* Our assumed input values are log2 transformed peptide ion intensities, which are stored in a 2d numpy array called \"samples\". Each row in samples represents a peptide and each column represents a sample\n",
    "\n",
    "* In a first step, we determine the all pairwise distances between the samples (details explained below)\n",
    "* We then choose the pair of samples with the closest distance between each other\n",
    "* We randomly choose one \"anchor\" sample and one \"shift\" sample and we subtract the distance between the samples from each peptide intensity measured in the \"shift\" sample. This is equivalent to rescaling the intensities of the original sample by a constant factor such that the distributions are aligned\n",
    "* We then construct a virtual \"merged\" sample by computing the average intensities of anchor and shift sample\n",
    "* We repeat the steps above until all samples are merged. Keeping track of the shift factors allows us then to determine an ideal shift for each sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_normfacts_withincond(samples):##row is the sample column is the features\n",
    "\n",
    "    \"finds optimal scaling factors for samples measured in the same condition and corrects the samples by these scaling factors. Takes a 2d numpy array as input  \"\n",
    "    num_samples = samples.shape[0]\n",
    "    mergedsamples = np.copy(samples) #the virtual \"merged\" samples will be stored in this array\n",
    "    sampleidx2shift = dict(zip(range(num_samples), np.zeros(num_samples))) #the scaling factors applied to the samples are stored here\n",
    "    sampleidx2counts = dict(zip(range(num_samples), np.ones(num_samples)))#keeps track of how many distributions are merged\n",
    "    sampleidx2anchoridx = {} #keeps track of the shifted samples\n",
    "    exclusion_set = set() #already clustered samples are stored here\n",
    "    distance_matrix = create_distance_matrix(samples)\n",
    "    variance_matrix = create_distance_matrix(samples, metric = 'variance')\n",
    "    #print(f\"distance matrix start\\n{distance_matrix}\")\n",
    "\n",
    "    for rep in range(num_samples-1):\n",
    "        #anchor_idx, shift_idx, min_distance = get_bestmatch_pair(mergedsamples, exclusion_set, sampleidx2counts)\n",
    "        anchor_idx, shift_idx, min_distance = get_bestmatch_pair(distance_matrix,variance_matrix, sampleidx2counts)\n",
    "        \n",
    "        # #determine the closest pair of samples (one \"shift\" sample to be shifted and one \"anchor sample which stays the same\") and the distance between this pair\n",
    "        #update the sets\n",
    "\n",
    "        if(anchor_idx == None):\n",
    "            break\n",
    "        sampleidx2anchoridx.update({shift_idx : anchor_idx})\n",
    "        sampleidx2shift.update({shift_idx : min_distance })\n",
    "        exclusion_set.add(shift_idx)\n",
    "\n",
    "        anchor_sample = mergedsamples[anchor_idx]\n",
    "        shift_sample = samples[shift_idx]\n",
    "        shifted_sample = shift_sample + min_distance\n",
    "\n",
    "        #print(f\"\\n\\nanchor {anchor_sample}\\nshift {shift_sample}\\nshifted sample{shifted_sample}\\nshiftfactor {min_distance}\\tsamples {shift_idx}\\t{anchor_idx}\")\n",
    "        merged_sample = merge_distribs(anchor_sample, shifted_sample, sampleidx2counts[anchor_idx], sampleidx2counts[shift_idx])\n",
    "        mergedsamples[anchor_idx] = merged_sample\n",
    "\n",
    "        #print(f\"shift, anchor: {shift_idx}, {anchor_idx} achtual shift {distance_matrix[shift_idx][anchor_idx]} or {distance_matrix[anchor_idx][shift_idx]}\")\n",
    "        #print(f\"distance matrix before\\n{distance_matrix}\")\n",
    "\n",
    "\n",
    "        update_distance_matrix(variance_matrix, mergedsamples, anchor_idx, shift_idx, metric='variance')\n",
    "        update_distance_matrix(distance_matrix, mergedsamples, anchor_idx, shift_idx)\n",
    "\n",
    "        #print(f\"distance matrix after\\n{distance_matrix}\")\n",
    "        sampleidx2counts[anchor_idx]+=1\n",
    "\n",
    "    sampleidx2totalshift = {}\n",
    "    for i in exclusion_set:\n",
    "        shift = get_total_shift(sampleidx2anchoridx, sampleidx2shift, i)\n",
    "        sampleidx2totalshift[i] = shift\n",
    "        #samples[i] = samples[i]+shift\n",
    "    return sampleidx2totalshift\n",
    "    #return samples\n",
    "\n",
    "def apply_sampleshifts(samples, sampleidx2shift):\n",
    "    for idx in sampleidx2shift.keys():\n",
    "        samples[idx] = samples[idx] + sampleidx2shift.get(idx)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best matching pair\n",
    "Take all pairs of the columns in the \"samples\" array that have not been already merged and compute the distance between the pairs as follows:\n",
    "* Subtract sample1 from sample2 (or sample2 from sample1, the order does not matter)\n",
    "* This results in a distribution of differences. As the samples array contains log2 intensities, this corresponds to taking log2 fold changes\n",
    "* Take the median of the distribution, this is a good approximation for the change between the two distributions\n",
    "* Select the two samples with the lowest absolute change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bestmatch_pair(distance_matrix, variance_matrix, sample2counts):\n",
    "    \n",
    "    i,j = np.unravel_index(np.argmin(variance_matrix, axis=None), variance_matrix.shape)\n",
    "    min_distance = distance_matrix[i,j]\n",
    "    #print(f\"idxs are {i}, {j} median is {distance_matrix[i][j]} variance is {variance_matrix[i][j]}\")\n",
    "    if(min_distance == np.inf):\n",
    "        return None, None, None\n",
    "    anchor_idx, shift_idx, min_distance = determine_anchor_and_shift_sample(sample2counts,i, j, min_distance) #direction flip of distance if necessary\n",
    "    return anchor_idx, shift_idx, min_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_distance_matrix(samples, metric = 'median'):\n",
    "    num_samples = samples.shape[0]\n",
    "    distance_matrix = np.full((num_samples, num_samples), np.inf)\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i+1, num_samples):#do every comparison once\n",
    "            distance_matrix[i,j] = calc_distance(metric, samples[i], samples[j]) #the median of the shifted distribution is taken as the distance measure\n",
    "            \n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_distance(metric, samples_1, samples_2):\n",
    "    res = None\n",
    "\n",
    "    if metric == 'median':\n",
    "        res = np.nanmedian(get_fcdistrib(samples_1, samples_2))#the median of the shifted distribution is taken as the distance measure\n",
    "    if(metric == 'variance'):\n",
    "        fcdist = get_fcdistrib(samples_1, samples_2)\n",
    "        #if sum(~np.isnan(fcdist))<2:\n",
    "         #   return np.nan\n",
    "        res = np.nanvar(fcdist)\n",
    "    if res == None:\n",
    "        raise Exception(f\"distance metric {metric} not implemented\")\n",
    "    if(np.isnan(res)):\n",
    "        return np.inf\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_distance_matrix(distance_matrix, merged_samples, merged_sample_idx, shift_idx,metric ='median'):\n",
    "    \"determine the distances to the newly merged sample\"\n",
    "    for i in range(0, merged_sample_idx):#update rows of distance matrix\n",
    "        if distance_matrix[i, merged_sample_idx]==np.inf:#do not compare already merged samples\n",
    "            continue\n",
    "        distance = calc_distance(metric,merged_samples[i], merged_samples[merged_sample_idx])\n",
    "        distance_matrix[i, merged_sample_idx] = distance\n",
    "    \n",
    "    for j in range(merged_sample_idx+1, merged_samples.shape[0]):#update columns of distance matrix\n",
    "        if distance_matrix[merged_sample_idx, j] == np.inf:\n",
    "            continue\n",
    "        distance = calc_distance(metric,merged_samples[merged_sample_idx], merged_samples[j])\n",
    "        distance_matrix[merged_sample_idx, j] = distance\n",
    "    \n",
    "    distance_matrix[shift_idx] = np.inf #shifted samples are excluded by setting distance to infinity\n",
    "    distance_matrix[:, shift_idx] = np.inf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_fcdistrib(logvals_rep1, logvals_rep2):\n",
    "    \"generates difference distribution between two samples\"\n",
    "    dist = np.subtract(logvals_rep1, logvals_rep2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def determine_anchor_and_shift_sample(sample2counts, i_min, j_min, min_distance):\n",
    "    \"given two samples, declare the sample with fewer merges as the shift\"\n",
    "    counts_i = sample2counts[i_min]\n",
    "    counts_j = sample2counts[j_min]\n",
    "    anchor_idx = i_min if counts_i>=counts_j else j_min\n",
    "    shift_idx = j_min if anchor_idx == i_min else i_min\n",
    "    flip = 1 if anchor_idx == i_min else -1\n",
    "    return anchor_idx, shift_idx, flip*min_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting samples\n",
    "When we have computed the distance between two samples, we want to correct one of the samples by this distance. This results in two distributions with the same median value. We always shift the sample which has been merged from fewer distributions (see below for details). The sample to which the shift is applied is call \"shift\" sample and the sample which is not shifted is called \"anchor\" sample.\n",
    "A \"total shift\" is calculated after all samples are merged, just by following up how many shifts have been applied to a sample in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def shift_samples(samples, sampleidx2anchoridx, sample2shift):\n",
    "    for sample_idx in range(samples.shape[0]):\n",
    "        samples[sample_idx] = samples[sample_idx]+get_total_shift(sampleidx2anchoridx, sample2shift, sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_total_shift(sampleidx2anchoridx, sample2shift,sample_idx):\n",
    "\n",
    "    total_shift = 0.0\n",
    "\n",
    "    while(True):\n",
    "        total_shift +=sample2shift[sample_idx]\n",
    "        if sample_idx not in sampleidx2anchoridx: #every shifted sample has an anchor\n",
    "            break\n",
    "        sample_idx = sampleidx2anchoridx[sample_idx]\n",
    "\n",
    "    return total_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging distributions\n",
    "After we shift two distributions on top of each other, we calculate a \"merged\" distribution. Each intensity in the merged distribution is the average of the intensity in both distributions. For the merging we have to take into account the following: If for example the anchor sample has already been merged from 10 samples, and the shift distribution has not been merged at all, we want to weigh the distribution coming from many samples higher. We hence multiply each sample by the number of merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def merge_distribs(anchor_distrib, shifted_distrib,counts_anchor_distrib, counts_shifted_distrib):\n",
    "    \"Calculate the average peptide intensities to merge two peptide distributions\"\n",
    "\n",
    "    t_alt = time.time()\n",
    "    res = np.zeros(len(anchor_distrib))\n",
    "\n",
    "    nans_anchor = np.isnan(anchor_distrib)\n",
    "    nans_shifted = np.isnan(shifted_distrib)\n",
    "    nans_anchor_and_shifted = nans_anchor & nans_shifted\n",
    "    nans_only_anchor = nans_anchor & ~nans_shifted\n",
    "    nans_only_shifted = nans_shifted &~nans_anchor\n",
    "    no_nans = ~nans_anchor & ~nans_shifted\n",
    "\n",
    "    idx_anchor_and_shifted = np.where(nans_anchor_and_shifted)\n",
    "    idx_only_anchor = np.where(nans_only_anchor)\n",
    "    idx_only_shifted = np.where(nans_only_shifted)\n",
    "    idx_no_nans = np.where(no_nans)\n",
    "\n",
    "    res[idx_anchor_and_shifted] = np.nan\n",
    "    res[idx_only_anchor] = shifted_distrib[idx_only_anchor]\n",
    "    res[idx_only_shifted] = anchor_distrib[idx_only_shifted]\n",
    "    res[idx_no_nans] = (anchor_distrib[idx_no_nans] *counts_anchor_distrib + shifted_distrib[idx_no_nans]*counts_shifted_distrib)/(counts_anchor_distrib+counts_shifted_distrib)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import scipy.stats\n",
    "def determine_mode_iteratively(fcs):\n",
    "    \n",
    "    fcs = np.sort(fcs)\n",
    "    #cut away the most extreme fold changes\n",
    "    cumul_counts = np.linspace(0, len(fcs), len(fcs))\n",
    "    cumul_counts_rel = cumul_counts/cumul_counts[-1]\n",
    "    thresh = 0.05\n",
    "    subset_vec = (cumul_counts_rel>thresh/2) & (cumul_counts_rel<1-thresh/2)\n",
    "    fcs_subset = fcs[subset_vec]\n",
    "\n",
    "    window_size = (max(fcs_subset) - min(fcs_subset))/2\n",
    "\n",
    "    while len(fcs_subset)> 40:\n",
    "        new_fc_range = []\n",
    "        maximum_fcs = -1\n",
    "        i=0\n",
    "        while i<= (len(fcs_subset) - window_size): #sliding window through the fold changes\n",
    "            fc_lower = fcs_subset[i]\n",
    "            fc_upper = fc_lower + window_size\n",
    "            fc_upper_idx = find_nearest(fcs_subset, fc_upper)\n",
    "            number_fcs_in_interval = fc_upper_idx - i\n",
    "            if number_fcs_in_interval > maximum_fcs:\n",
    "                maximum_fcs = number_fcs_in_interval\n",
    "                new_fc_range = [i, fc_upper_idx]\n",
    "            i+=1\n",
    "        fcs_subset = fcs_subset[new_fc_range[0]:new_fc_range[1]]\n",
    "        window_size = window_size/2\n",
    "    #fc_selected = scipy.stats.mode(fcs_subset).mode[0]\n",
    "    fc_selected = np.median(fcs_subset)\n",
    "    return fc_selected\n",
    "\n",
    "\n",
    "def find_nearest(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "\n",
    "def mode_normalization(x):\n",
    "\n",
    "    x = np.sort(x)\n",
    "    \n",
    "    #cut away the most extreme fold changes\n",
    "    cumul_counts = np.linspace(0, len(x), len(x))\n",
    "    cumul_counts_rel = cumul_counts/cumul_counts[-1]\n",
    "    thresh = 0.05\n",
    "    subset_vec = (cumul_counts_rel>thresh/2) & (cumul_counts_rel<1-thresh/2)\n",
    "    x = x[subset_vec]\n",
    "\n",
    "    x_min = min(x)\n",
    "    x_max = max(x)\n",
    "    num_bins = int((x_max-x_min)*50)\n",
    "    bins = np.linspace(x_min, x_max, num_bins)\n",
    "    hist = np.histogram(x, bins)\n",
    "    x = hist[0]\n",
    "    fcs =  0.5*(hist[1][1:]+hist[1][:-1]) #get middle of each fc bin\n",
    "    cumul_x = np.cumsum(x)\n",
    "\n",
    "    peaks2  = find_peaks(x, prominence=1)\n",
    "    lbase = peaks2[1]['left_bases']\n",
    "    rbase = peaks2[1]['right_bases']\n",
    "    cumul_heights = cumul_x[rbase] - cumul_x[lbase]\n",
    "    max_cumul_idx = np.argmax(cumul_heights)#returns the peak with the highest probabilty mass\n",
    "    max_idx = peaks2[0][max_cumul_idx]\n",
    "    shift_fc = hist[1][max_idx]\n",
    "\n",
    "    return shift_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def get_betweencond_shift(df_c1_normed, df_c2_normed, enfore_median = False):\n",
    "\n",
    "    both_idx = df_c1_normed.index.intersection(df_c2_normed.index)\n",
    "    df1 = df_c1_normed.loc[both_idx]\n",
    "    df2 = df_c2_normed.loc[both_idx]\n",
    "    df1 = df1.median(axis = 1, skipna = True).to_frame()\n",
    "    df2 = df2.median(axis = 1, skipna = True).to_frame()\n",
    "    col1 = df1.columns[0]\n",
    "    col2 = df2.columns[0]\n",
    "\n",
    "    diff_fcs = df1[col1].to_numpy() - df2[col2].to_numpy()\n",
    "    median = np.nanmedian(diff_fcs)\n",
    "    if enfore_median:\n",
    "        return -median\n",
    "        \n",
    "    if len(diff_fcs)<100:\n",
    "        print(\"using median for shift\")\n",
    "        return -median\n",
    "    mode = mode_normalization(diff_fcs)\n",
    "    #mode = determine_mode_iteratively(diff_fcs)\n",
    "    print(f\"median {median}, mode {mode}\")\n",
    "    if(abs(median-mode) <0.05):\n",
    "        print(f\"using median for shift\")\n",
    "        return -median\n",
    "    else:\n",
    "        print(f\"using mode for shift\")\n",
    "        return -mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import directlfq.visualizations as aqviz\n",
    "def normalize_if_specified(df_c1, df_c2, c1_samples, c2_samples, minrep, normalize_within_conds = True, normalize_between_conds = True, runtime_plots = True, protein_subset_for_normalization_file = None, pep2prot =None,prenormed_file = None): #labelmap_df, unnormed_df,condpair,\n",
    "\n",
    "\n",
    "    if prenormed_file is not None:\n",
    "        return use_benchmark_prenormed_file(prenormed_file=prenormed_file, minrep = minrep, c1_samples = c1_samples, c2_samples = c2_samples)\n",
    "\n",
    "\n",
    "    if normalize_within_conds:\n",
    "        df_c1 = normalize_within_cond(df_c=df_c1, samples_c= c1_samples)\n",
    "        df_c2 = normalize_within_cond(df_c=df_c2, samples_c=c2_samples)\n",
    "        print(f\"normalized within conditions\")\n",
    "    \n",
    "    if runtime_plots:\n",
    "        plot_withincond_normalization(df_c1, df_c2)\n",
    "    \n",
    "    if normalize_between_conds:\n",
    "        df_c1, df_c2 = get_normalized_dfs_between_conditions(df_c1, df_c2, protein_subset_for_normalization_file, pep2prot,runtime_plots = runtime_plots)\n",
    "        print(\"normalized between conditions\")\n",
    "    \n",
    "    return df_c1, df_c2\n",
    "\n",
    "\n",
    "\n",
    "def get_normalized_dfs_between_conditions(df_c1, df_c2, protein_subset_for_normalization_file, pep2prot,runtime_plots):\n",
    "    shift_between_cond = prepare_tables_and_get_betweencond_shift(df_c1, df_c2, protein_subset_for_normalization_file, pep2prot)\n",
    "\n",
    "    print(f\"shift comparison by {shift_between_cond}\")\n",
    "    df_c2 = df_c2-shift_between_cond\n",
    "    #compare_normalization(\"./test_data/normed_intensities.tsv\", df_c1_normed, df_c2_normed)\n",
    "    if runtime_plots:\n",
    "        aqviz.plot_betweencond_fcs(df_c1, df_c2, False)\n",
    "        aqviz.plot_betweencond_fcs(df_c1, df_c2, True)\n",
    "    return df_c1, df_c2\n",
    "\n",
    "def normalize_within_cond(df_c, samples_c):\n",
    "    sample2shift = get_normfacts_withincond(drop_nas_if_possible(df_c).to_numpy().T)\n",
    "    df_c_normed = pd.DataFrame(apply_sampleshifts(df_c.to_numpy().T, sample2shift).T, index = df_c.index, columns = samples_c)\n",
    "    return df_c_normed\n",
    "\n",
    "def prepare_tables_and_get_betweencond_shift(df_c1, df_c2, protein_subset_for_normalization_file, pep2prot):\n",
    "    specified_protein_subset = read_specified_protein_subset_if_given(protein_subset_for_normalization_file)\n",
    "    prepared1 = prepare_table_for_betweencond_shift(df_c1, specified_protein_subset, pep2prot)\n",
    "    prepared2 = prepare_table_for_betweencond_shift(df_c2, specified_protein_subset, pep2prot)\n",
    "    enforce_median = protein_subset_for_normalization_file is not None\n",
    "    return get_betweencond_shift(prepared1, prepared2, enforce_median)\n",
    "\n",
    "def read_specified_protein_subset_if_given(specified_protein_subset_file):\n",
    "    if specified_protein_subset_file is not None:\n",
    "        return get_protein_subset_from_protein_list(pd.read_csv(specified_protein_subset_file, sep = \"\\t\")[\"protein\"])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_protein_subset_from_protein_list(protein_list):\n",
    "    protein_subset = []\n",
    "    for proteingroup in protein_list:\n",
    "        for protein in proteingroup.split(\";\"):\n",
    "            protein_subset.append(protein)\n",
    "    \n",
    "    return set(protein_subset)\n",
    "\n",
    "\n",
    "def prepare_table_for_betweencond_shift(df, specified_protein_subset, pep2prot):\n",
    "    filtered_df = filter_to_protein_subset(df, specified_protein_subset, pep2prot)\n",
    "    filtered_df = drop_nas_if_possible(filtered_df)\n",
    "    return filtered_df\n",
    "    \n",
    "\n",
    "def filter_to_protein_subset(df, specified_protein_subset, pep2prot):\n",
    "    if specified_protein_subset is None:\n",
    "        return df\n",
    "    else:\n",
    "        proteins = [pep2prot.get(x) for x in df.index]\n",
    "        return df[[test_if_proteingroup_is_in_subset(x, specified_protein_subset) for x in proteins]] #protein is set to index\n",
    "\n",
    "def test_if_proteingroup_is_in_subset(proteingroup, specified_protein_subset):\n",
    "    for protein in proteingroup.split(\";\"):\n",
    "        if protein in specified_protein_subset:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def drop_nas_if_possible(df):\n",
    "    df_nonans = df.dropna(axis=0)\n",
    "    fraction_nonans = calculate_fraction_with_no_NAs(df, df_nonans)\n",
    "    if fraction_nonans<0.05:\n",
    "        print('to few values for normalization without missing values. Including missing values')\n",
    "        return df\n",
    "    else:\n",
    "        return df_nonans\n",
    "\n",
    "def calculate_fraction_with_no_NAs(df, df_nonnans):\n",
    "    return len(df_nonnans.index)/len(df.index)\n",
    "\n",
    "\n",
    "def plot_withincond_normalization(df_c1, df_c2):\n",
    "    print(\"without missingvals (if applicable)\")\n",
    "    aqviz.plot_betweencond_fcs(drop_nas_if_possible(df_c1), drop_nas_if_possible(df_c2), True)\n",
    "    print(\"complete dataset\")\n",
    "    aqviz.plot_betweencond_fcs(df_c1, df_c2, True)\n",
    "\n",
    "def use_benchmark_prenormed_file(prenormed_file, minrep, c1_samples, c2_samples):\n",
    "    print(\"using pre-normalized data - skipping normalization\")\n",
    "    df_prenormed = pd.read_csv(prenormed_file, sep=\"\\t\",index_col = \"ion\")\n",
    "    df_c1_normed = df_prenormed[c1_samples].dropna(thresh=minrep, axis=0)\n",
    "    df_c2_normed = df_prenormed[c2_samples].dropna(thresh=minrep, axis=0)\n",
    "    df_c2_normed = df_c2_normed +0.18\n",
    "    return df_c1_normed, df_c2_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def test_merged_distribs():\n",
    "    anchor_distrib = np.array([1, 1, 1, 1, 1])\n",
    "    shift_distrib = np.array([2, 2, 2, 2, 2])\n",
    "    counts_anchor_distrib = 4\n",
    "    counts_shifted_distib = 1\n",
    "    assert (merge_distribs(anchor_distrib, shift_distrib, counts_anchor_distrib, counts_shifted_distib)== np.array([1.2, 1.2, 1.2, 1.2, 1.2])).any()\n",
    "\n",
    "test_merged_distribs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def generate_randarrays(number_arrays,size_of_array):\n",
    "    randarray = []\n",
    "    for i in range(number_arrays):\n",
    "        shift = np.random.uniform(low=-10, high=+10)\n",
    "        randarray.append(np.random.normal(loc=shift, size=size_of_array))\n",
    "    return np.array(randarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDev 1.0076066494260316\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_sampleshift(samples):\n",
    "    num_samples = samples.shape[0]\n",
    "    merged_sample = []\n",
    "    for i in range(num_samples):\n",
    "        plt.hist(samples[i])\n",
    "        merged_sample.extend(samples[i])\n",
    "    stdev = np.std(merged_sample)\n",
    "    print(f\"STDev {stdev}\")\n",
    "    assert (stdev <=1.2) \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "randarray = generate_randarrays(5, 1000)\n",
    "sample2shift = get_normfacts_withincond(randarray)\n",
    "normalized_randarray = apply_sampleshifts(randarray, sample2shift)\n",
    "test_sampleshift(normalized_randarray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
