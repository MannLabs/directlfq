{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: normalization.html\n",
    "title: Normalization\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-condition normalization\n",
    "It is common practice and highly recommended to measure multiple samples of a given condition. This ensures that observed changes between conditions are not just due to random variation. Examples of samples within the same condition could be biological replicates, but also patients with the same clinical condition. \n",
    "We want to ensure that systematic changes between within-condition samples are corrected for as follows:\n",
    "\n",
    "* Our assumed input values are log2 transformed peptide ion intensities, which are stored in a 2d numpy array called \"samples\". Each row in samples represents a peptide and each column represents a sample\n",
    "\n",
    "* In a first step, we determine the all pairwise distances between the samples (details explained below)\n",
    "* We then choose the pair of samples with the closest distance between each other\n",
    "* We randomly choose one \"anchor\" sample and one \"shift\" sample and we subtract the distance between the samples from each peptide intensity measured in the \"shift\" sample. This is equivalent to rescaling the intensities of the original sample by a constant factor such that the distributions are aligned\n",
    "* We then construct a virtual \"merged\" sample by computing the average intensities of anchor and shift sample\n",
    "* We repeat the steps above until all samples are merged. Keeping track of the shift factors allows us then to determine an ideal shift for each sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best matching pair\n",
    "Take all pairs of the columns in the \"samples\" array that have not been already merged and compute the distance between the pairs as follows:\n",
    "* Subtract sample1 from sample2 (or sample2 from sample1, the order does not matter)\n",
    "* This results in a distribution of differences. As the samples array contains log2 intensities, this corresponds to taking log2 fold changes\n",
    "* Take the median of the distribution, this is a good approximation for the change between the two distributions\n",
    "* Select the two samples with the lowest absolute change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting samples\n",
    "When we have computed the distance between two samples, we want to correct one of the samples by this distance. This results in two distributions with the same median value. We always shift the sample which has been merged from fewer distributions (see below for details). The sample to which the shift is applied is call \"shift\" sample and the sample which is not shifted is called \"anchor\" sample.\n",
    "A \"total shift\" is calculated after all samples are merged, just by following up how many shifts have been applied to a sample in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging distributions\n",
    "After we shift two distributions on top of each other, we calculate a \"merged\" distribution. Each intensity in the merged distribution is the average of the intensity in both distributions. For the merging we have to take into account the following: If for example the anchor sample has already been merged from 10 samples, and the shift distribution has not been merged at all, we want to weigh the distribution coming from many samples higher. We hence multiply each sample by the number of merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift linear to reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import numpy as np\n",
    "import directlfq.normalization as lfq_norm\n",
    "\n",
    "def test_merged_distribs():\n",
    "    anchor_distrib = np.array([1, 1, 1, 1, 1])\n",
    "    shift_distrib = np.array([2, 2, 2, 2, 2])\n",
    "    counts_anchor_distrib = 4\n",
    "    counts_shifted_distib = 1\n",
    "    assert (lfq_norm.merge_distribs(anchor_distrib, shift_distrib, counts_anchor_distrib, counts_shifted_distib)== np.array([1.2, 1.2, 1.2, 1.2, 1.2])).any()\n",
    "\n",
    "\n",
    "test_merged_distribs() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import directlfq.normalization as lfq_norm\n",
    "\n",
    "\n",
    "def test_order_of_shifts():\n",
    "    vals1 = [1, np.nan, 1.5]\n",
    "    vals2 = [1, 1, np.nan]\n",
    "    vals3 = [3.2, 1, 2.8]\n",
    "    vals4 = [4.2, 2, 3.8]\n",
    "    list_of_vals = [vals1, vals2, vals3, vals4]\n",
    "    protein_profile_df = create_input_df_from_input_vals(list_of_vals)\n",
    "    display(protein_profile_df)\n",
    "    protein_profile_numpy = protein_profile_df.to_numpy()\n",
    "    sample2shift = lfq_norm.get_normfacts(protein_profile_numpy)\n",
    "    assert sample2shift == {1: 0.0, 2: -1.2999999999999998, 3: -2.3}\n",
    "    print(lfq_norm.create_distance_matrix(protein_profile_numpy, metric = 'variance'))\n",
    "    \n",
    "    df_normed = pd.DataFrame(lfq_norm.apply_sampleshifts(protein_profile_numpy, sample2shift), index = protein_profile_df.index, columns = protein_profile_df.columns)\n",
    "    display(df_normed)\n",
    "\n",
    "def create_input_df_from_input_vals(list_of_vals):\n",
    "    index_vals = [(\"A\", f\"ion{x}\") for x in range(len(list_of_vals))]\n",
    "    index = pd.Index(index_vals, name=('protein', 'ion'))\n",
    "    return pd.DataFrame(list_of_vals, index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_order_of_shifts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import directlfq.normalization as lfq_norm\n",
    "\n",
    "def generate_randarrays(number_arrays,size_of_array):\n",
    "    randarray = []\n",
    "    for i in range(number_arrays):\n",
    "        shift = np.random.uniform(low=-10, high=+10)\n",
    "        randarray.append(np.random.normal(loc=shift, size=size_of_array))\n",
    "    return np.array(randarray)\n",
    "\n",
    "def test_sampleshift(samples):\n",
    "    num_samples = samples.shape[0]\n",
    "    merged_sample = []\n",
    "    for i in range(num_samples):\n",
    "        plt.hist(samples[i])\n",
    "        merged_sample.extend(samples[i])\n",
    "    stdev = np.std(merged_sample)\n",
    "    print(f\"STDev {stdev}\")\n",
    "    assert (stdev <=1.2) \n",
    "    \n",
    "    plt.show()\n",
    "randarray = generate_randarrays(5, 1000)\n",
    "sample2shift = lfq_norm.get_normfacts(randarray)\n",
    "normalized_randarray = lfq_norm.apply_sampleshifts(randarray, sample2shift)\n",
    "test_sampleshift(normalized_randarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import directlfq.visualizations as lfq_viz\n",
    "import directlfq.utils as lfq_utils\n",
    "import directlfq.normalization as lfq_norm\n",
    "\n",
    "def test_normalizing_between_samples(num_samples_quadratic):\n",
    "    input_file = \"../test_data/unit_tests/protein_normalization/peptides.txt.maxquant_peptides_benchmarking.aq_reformat.tsv\"\n",
    "    input_df = pd.read_csv(input_file, sep = '\\t')\n",
    "    input_df = lfq_utils.index_and_log_transform_input_df(input_df)\n",
    "    input_df = input_df[[x for x in input_df.columns if \"Shotgun\" in x]]\n",
    "    lfq_viz.plot_withincond_fcs(input_df)\n",
    "    input_df_normalized = lfq_norm.NormalizationManagerSamples(input_df, num_samples_quadratic=num_samples_quadratic).complete_dataframe\n",
    "    lfq_viz.plot_withincond_fcs(input_df_normalized)\n",
    "    assert_that_results_scatter_around_zero(input_df_normalized)\n",
    "\n",
    "\n",
    "def assert_that_results_scatter_around_zero(input_df_normalized):\n",
    "    median_intensities = input_df_normalized.median(axis=1)\n",
    "    input_df_subtracted = input_df_normalized.subtract(median_intensities, axis=0)\n",
    "    median_of_medians = input_df_subtracted.median(axis=0)\n",
    "    assert (median_of_medians < 0.1).all()\n",
    "    print(\"checked that close to zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_normalizing_between_samples(100)\n",
    "test_normalizing_between_samples(3)\n",
    "test_normalizing_between_samples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import directlfq.normalization as lfq_norm\n",
    "import directlfq.test_utils as lfq_test_utils\n",
    "import numpy as np\n",
    "\n",
    "def test_that_profiles_without_noise_are_shifted_exactly_on_top_of_each_other():\n",
    "    peptide1= lfq_test_utils.PeptideProfile(protein_name=\"protA\", fraction_zeros_in_profile=0.1, systematic_peptide_shift=3000, add_noise=False)\n",
    "    peptide2= lfq_test_utils.PeptideProfile(protein_name=\"protA\",fraction_zeros_in_profile=0.9, systematic_peptide_shift=3, add_noise=False)\n",
    "    peptide3= lfq_test_utils.PeptideProfile(protein_name=\"protA\", fraction_zeros_in_profile=0.1, systematic_peptide_shift=0.1, add_noise=False)\n",
    "    peptide4= lfq_test_utils.PeptideProfile(protein_name=\"protA\",fraction_zeros_in_profile=0.9, systematic_peptide_shift=100, add_noise=False)\n",
    "    protein_df = lfq_test_utils.ProteinProfileGenerator([peptide1, peptide2, peptide3, peptide4]).protein_profile_dataframe\n",
    "    display(protein_df)\n",
    "    normed_ion_profile = lfq_norm.normalize_ion_profiles(protein_df)\n",
    "    display(normed_ion_profile)\n",
    "    column_from_shifted = normed_ion_profile.iloc[:,11].dropna().to_numpy()\n",
    "    display(column_from_shifted)\n",
    "    assert np.allclose(column_from_shifted, column_from_shifted[0])\n",
    "\n",
    "def test_that_profiles_with_noise_are_close():\n",
    "    peptide1= lfq_test_utils.PeptideProfile(protein_name=\"protA\", fraction_zeros_in_profile=0, systematic_peptide_shift=3000, add_noise=True)\n",
    "    peptide2= lfq_test_utils.PeptideProfile(protein_name=\"protA\",fraction_zeros_in_profile=0, systematic_peptide_shift=3, add_noise=True)\n",
    "    peptide3= lfq_test_utils.PeptideProfile(protein_name=\"protA\", fraction_zeros_in_profile=0, systematic_peptide_shift=0.1, add_noise=True)\n",
    "    peptide4= lfq_test_utils.PeptideProfile(protein_name=\"protA\",fraction_zeros_in_profile=0, systematic_peptide_shift=100, add_noise=True)\n",
    "\n",
    "    protein_df = lfq_test_utils.ProteinProfileGenerator([peptide1, peptide2, peptide3, peptide4]).protein_profile_dataframe\n",
    "    display(protein_df)\n",
    "    \n",
    "    normed_ion_profile = lfq_norm.normalize_ion_profiles(protein_df)\n",
    "    display(normed_ion_profile)\n",
    "    column_from_shifted = normed_ion_profile.iloc[:,9].dropna().to_numpy()\n",
    "\n",
    "    assert np.allclose(column_from_shifted, column_from_shifted[0],rtol=0.01, atol=0.01)\n",
    "\n",
    "test_that_profiles_without_noise_are_shifted_exactly_on_top_of_each_other()\n",
    "test_that_profiles_with_noise_are_close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import directlfq.normalization as lfq_norm\n",
    "import numpy as np\n",
    "\n",
    "def _calc_distance(samples_1, samples_2):\n",
    "    distrib = lfq_norm.get_fcdistrib(samples_1, samples_2)\n",
    "    is_all_nan = np.all(np.isnan(distrib))\n",
    "    if is_all_nan:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.nanmedian(distrib)\n",
    "\n",
    "def test_calc_distance():\n",
    "    print(\"Test 2: One array is entirely NaN\")\n",
    "    samples_1 = np.array([np.nan, np.nan, np.nan])\n",
    "    samples_2 = np.array([1, 2, 3])\n",
    "    assert np.isnan(lfq_norm.SampleShifterLinear._calc_distance(samples_1, samples_2)) == True, \"Test 2 failed: Expected NaN for an array entirely of NaNs\"\n",
    "    \n",
    "\n",
    "    print(\"Test 1: Both arrays are non-NaN and identical\")\n",
    "    samples_1 = np.array([1, 2, 3])\n",
    "    samples_2 = np.array([1, 2, 3])\n",
    "    assert np.isnan(_calc_distance(samples_1, samples_2)) == False, \"Test 1 failed: Expected a non-NaN result for identical arrays\"\n",
    "    assert lfq_norm.SampleShifterLinear._calc_distance(samples_1, samples_2) == 0, \"Test 1 failed: Expected a distance of 0 for identical arrays\"\n",
    "    \n",
    "\n",
    "    print(\"Test 3: Arrays with some NaN values\")\n",
    "    samples_1 = np.array([1, np.nan, 3])\n",
    "    samples_2 = np.array([13, 2, np.nan])\n",
    "    assert np.isnan(_calc_distance(samples_1, samples_2)) == False, \"Test 3 failed: Expected a valid number even with some NaNs\"\n",
    "    assert lfq_norm.SampleShifterLinear._calc_distance(samples_1, samples_2) == -12 , \"Test 3 failed: Expected a distance of -12\"\n",
    "    \n",
    "    print(\"Test 4: Arrays with different values but no NaNs\")\n",
    "    samples_1 = np.array([1, 4, 7])\n",
    "    samples_2 = np.array([2, 5, 8])\n",
    "    assert lfq_norm.SampleShifterLinear._calc_distance(samples_1, samples_2) != 0, \"Test 4 failed: Expected a non-zero distance for different values\"\n",
    "    \n",
    "    print(\"Test 5: Empty arrays\")\n",
    "    samples_1 = np.array([])\n",
    "    samples_2 = np.array([])\n",
    "    assert np.isnan(lfq_norm.SampleShifterLinear._calc_distance(samples_1, samples_2)) == True, \"Test 5 failed: Expected NaN for empty arrays\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test function\n",
    "test_calc_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import directlfq.utils as lfq_utils\n",
    "import directlfq.normalization as lfq_norm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the number of samples and proteins\n",
    "num_samples = 200\n",
    "num_rows = 1000  # Example number of rows; adjust as necessary\n",
    "\n",
    "# Define column names for samples\n",
    "sample_columns = [f'sample_{i}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "# Create the DataFrame with protein and ion columns first\n",
    "df = pd.DataFrame(index=range(num_rows))\n",
    "df['protein'] = ['protein_' + str(i) for i in np.random.randint(1, 100, size=num_rows)]  # Random protein tags\n",
    "df['ion'] = ['ion_' + str(i) for i in range(1, num_rows + 1)]  # Unique ion tags\n",
    "\n",
    "# Add sample columns\n",
    "for col in sample_columns:\n",
    "    df[col] = np.random.normal(loc= 50,size=num_rows)  # Populate with log-normal distribution\n",
    "    \n",
    "    # Determine the fraction of zero values for the column (only for the lower half)\n",
    "    fraction_zeros = np.random.uniform(0.1, 0.9)\n",
    "    lower_half_indices = df.index[num_rows // 2:]  # Identifying the lower half of the DataFrame\n",
    "    zero_indices = np.random.choice(lower_half_indices, int(fraction_zeros * len(lower_half_indices)), replace=False)\n",
    "    df.loc[zero_indices, col] = 0\n",
    "    \n",
    "    # Multiply the column by a uniformly drawn factor between 1 and 15\n",
    "    multiplier = np.random.randint(10, 50)\n",
    "    df[col] *= multiplier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = lfq_utils.index_and_log_transform_input_df(df)\n",
    "display(df)\n",
    "\n",
    "df_normalized = lfq_norm.NormalizationManagerSamplesOnSelectedProteins(df.copy(), num_samples_quadratic=20).complete_dataframe\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_normalized_nona = df_normalized.dropna()\n",
    "display(df_normalized_nona)\n",
    "display(df.dropna())\n",
    "\n",
    "# Plot a boxplot for each 'sample_x' column\n",
    "plt.figure(figsize=(20, 10))  # Adjust the size as needed\n",
    "df_normalized_nona.boxplot(column=[f'sample_{i}' for i in range(1, num_samples + 1)])\n",
    "plt.xticks(rotation=90)  # Rotating the x labels for better readability\n",
    "plt.title('Boxplot of Log-transformed Intensities for Each Sample Column')\n",
    "plt.ylabel('Log2 Intensity')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot a boxplot for each 'sample_x' column\n",
    "plt.figure(figsize=(20, 10))  # Adjust the size as needed\n",
    "df.dropna().boxplot(column=[f'sample_{i}' for i in range(1, num_samples + 1)])\n",
    "plt.xticks(rotation=90)  # Rotating the x labels for better readability\n",
    "plt.title('Boxplot of Log-transformed Intensities for Each Sample Column')\n",
    "plt.ylabel('Log2 Intensity')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def test_taking_the_mean_along_an_axis():\n",
    "\n",
    "    example_set = sns.load_dataset(\"iris\").set_index(\"species\")\n",
    "\n",
    "    example_mean = example_set.mean(axis=1)\n",
    "\n",
    "    assert example_mean.to_numpy()[3] == np.mean([4.6, 3.1, 1.5, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_taking_the_mean_along_an_axis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "directlfq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
